{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOp91fgwdMYxoEO/RAt4Gfe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"GPRlcJhO8nya"},"outputs":[],"source":["# 구글드라이브 구글코랩에 연동(마운트)\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# Diffusion Model for Sensor Dataset Imputation\n","import os\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader, Dataset, random_split\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import zipfile\n","from sklearn.preprocessing import StandardScaler"],"metadata":{"id":"ybhre9xe9_Qh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Custom Dataset for Sensor Data\n","class SensorDataset(Dataset):\n","    def __init__(self, data_dir, transform=None):\n","        self.data_dir = data_dir\n","        self.transform = transform\n","\n","        # Load and process all sensor data\n","        self.data = self._load_data()\n","\n","    def _load_data(self):\n","        # Combine all sensor files into a single matrix (300 x 75,600)\n","        all_data = []\n","        for i in range(1, 64):\n","            file_path = os.path.join(self.data_dir, f\"ProcessVar{i}.csv\")\n","            if not os.path.exists(file_path):\n","                raise FileNotFoundError(f\"File not found: {file_path}\")\n","            sensor_data = pd.read_csv(file_path, header=None).values.T  # Transpose to 300x1200\n","            all_data.append(sensor_data)\n","        return np.hstack(all_data)  # Combine all sensors column-wise\n","\n","    def __len__(self):\n","        return self.data.shape[0]\n","\n","    def __getitem__(self, idx):\n","        sample = self.data[idx, :]\n","        if self.transform:\n","            sample = self.transform(sample)\n","        return sample"],"metadata":{"id":"Ro38W2oj9x7u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the 1D UNet for the diffusion process\n","class UNet1D(nn.Module):\n","    def __init__(self, input_dim):\n","        super(UNet1D, self).__init__()\n","        self.encoder1 = nn.Conv1d(1, 32, kernel_size=3, stride=1, padding=1)\n","        self.encoder2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)\n","        self.decoder1 = nn.ConvTranspose1d(64, 32, kernel_size=3, stride=1, padding=1)\n","        self.decoder2 = nn.ConvTranspose1d(32, 1, kernel_size=3, stride=1, padding=1)\n","\n","    def forward(self, x):\n","        x = x.unsqueeze(1)  # Add channel dimension for Conv1D\n","        enc1 = F.relu(self.encoder1(x))\n","        enc2 = F.relu(self.encoder2(enc1))\n","        dec1 = F.relu(self.decoder1(enc2))\n","        dec2 = self.decoder2(dec1)\n","        return dec2.squeeze(1)  # Remove channel dimension"],"metadata":{"id":"_-qySWsh95Q3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Noise schedule for diffusion\n","def linear_beta_schedule(timesteps):\n","    beta_start = 0.0001\n","    beta_end = 0.02\n","    return torch.linspace(beta_start, beta_end, timesteps)\n"],"metadata":{"id":"2e0Lj76c-FcC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Forward Diffusion Process\n","def forward_diffusion(x0, t, noise_schedule):\n","    noise = torch.randn_like(x0)\n","    sqrt_alpha_cumprod = torch.sqrt(1.0 - noise_schedule[:t].prod())\n","    sqrt_one_minus_alpha_cumprod = torch.sqrt(noise_schedule[:t].prod())\n","    return sqrt_alpha_cumprod * x0 + sqrt_one_minus_alpha_cumprod * noise, noise\n"],"metadata":{"id":"gxUiGhXz-G2T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loss Function for Reverse Process\n","class DiffusionLoss(nn.Module):\n","    def __init__(self):\n","        super(DiffusionLoss, self).__init__()\n","        self.mse = nn.MSELoss()\n","\n","    def forward(self, predicted_noise, true_noise):\n","        return self.mse(predicted_noise, true_noise)"],"metadata":{"id":"oTtTfP-d-IPY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training Loop for Diffusion Model\n","def train_diffusion(model, dataloader, optimizer, loss_fn, timesteps, noise_schedule, epochs):\n","    model.train()\n","    for epoch in range(epochs):\n","        total_loss = 0\n","        for x0 in dataloader:\n","            optimizer.zero_grad()\n","            t = torch.randint(1, timesteps + 1, (1,)).item()\n","            noisy_x, noise = forward_diffusion(x0.float(), t, noise_schedule)\n","            predicted_noise = model(noisy_x.float())\n","            loss = loss_fn(predicted_noise, noise)\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item()\n","        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader):.4f}\")"],"metadata":{"id":"wGz90Cv6-JsM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Generate Missing Data with Diffusion Model\n","def impute_missing_data(model, corrupted_data, timesteps, noise_schedule):\n","    model.eval()\n","    generated_data = corrupted_data.clone().float()\n","    for t in range(timesteps, 0, -1):\n","        with torch.no_grad():\n","            noise = model(generated_data)\n","            beta_t = noise_schedule[t - 1]\n","            sqrt_one_minus_beta_t = torch.sqrt(1 - beta_t)\n","            generated_data = (generated_data - beta_t * noise) / sqrt_one_minus_beta_t\n","    return generated_data"],"metadata":{"id":"vtQ5imAN-LSw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Main Execution\n","if __name__ == \"__main__\":\n","    # Mount Google Drive and extract the zip file\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    zip_path = \"/content/drive/MyDrive/Colab Notebooks/Generative_Deep_Learning_2nd_Edition/Generative_Deep_Learning_2nd_Edition/notebooks/08_diffusion/01_ddm/Sensor_data.zip\"\n","    extract_path = \"/content/Sensor_data\"\n","\n","    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","        zip_ref.extractall(extract_path)\n","\n","    # Load Dataset\n","    data_dir = extract_path\n","    dataset = SensorDataset(data_dir, transform=torch.tensor)\n","\n","    # Split into train and test sets\n","    train_size = int(0.8 * len(dataset))\n","    test_size = len(dataset) - train_size\n","    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n","\n","    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n","\n","    # Define Diffusion Model\n","    input_dim = dataset[0].shape[0]\n","    timesteps = 100\n","    noise_schedule = linear_beta_schedule(timesteps)\n","\n","    model = UNet1D(input_dim)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","    loss_fn = DiffusionLoss()\n","\n","    # Train Diffusion Model\n","    train_diffusion(model, train_loader, optimizer, loss_fn, timesteps, noise_schedule, epochs=20)\n","\n","    # Save the trained model\n","    torch.save(model.state_dict(), \"sensor_ddpm_model.pth\")\n","    print(\"Model saved as 'sensor_ddpm_model.pth'\")\n","\n","    # Impute Missing Data\n","    corrupted_data = torch.tensor(dataset.data).clone().float()  # Convert to PyTorch tensor\n","    corrupted_data[torch.rand_like(corrupted_data) < 0.1] = 0  # Mask 10% of the data\n","    generated_data = impute_missing_data(model, corrupted_data, timesteps, noise_schedule)\n","\n","    # Visualize Results\n","    for sensor_idx in range(5):\n","        plt.figure(figsize=(12, 4))\n","        plt.plot(dataset.data[sensor_idx], label=\"Real Data\", alpha=0.7, color=\"blue\")\n","        plt.plot(corrupted_data[sensor_idx], label=\"Corrupted Data\", alpha=0.7, color=\"orange\")\n","        plt.plot(generated_data[sensor_idx], label=\"Imputed Data\", alpha=0.7, color=\"green\")\n","        plt.title(f\"Sensor {sensor_idx + 1}: Real vs Corrupted vs Imputed\")\n","        plt.xlabel(\"Time Steps\")\n","        plt.ylabel(\"Sensor Values\")\n","        plt.legend()\n","        plt.show()\n","\n","    # Reload the model (example for future use)\n","    model.load_state_dict(torch.load(\"sensor_ddpm_model.pth\"))\n","    model.eval()\n","    print(\"Model reloaded for further experiments\")"],"metadata":{"id":"kqSieuJI-Mip"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9FqgQOwMoDTU"},"execution_count":null,"outputs":[]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNGCjGmOLBG15PrFaLm0KJQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ldYxoW_2sJ7j"},"outputs":[],"source":["# 구글드라이브 구글코랩에 연동(마운트)\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# Denoising Diffusion Model for Sensor Dataset Imputation\n","#!pip install scikit-learn\n","import os\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch.utils.data import DataLoader, Dataset, random_split\n","from torchvision.transforms import Compose\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt\n","import zipfile\n","from sklearn.preprocessing import StandardScaler\n"],"metadata":{"id":"MtAorEPzsTl9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"0CwvunI3uyw7"}},{"cell_type":"code","source":["# Custom Dataset for Sensor Data\n","class SensorDataset(Dataset):\n","    def __init__(self, data_dir, transform=None):\n","        self.data_dir = data_dir\n","        self.transform = transform\n","\n","        # Load and process all sensor data\n","        self.data = self._load_data()\n","\n","    def _load_data(self):\n","        # Combine all sensor files into a single matrix (300 x 75,600)\n","        all_data = []\n","        for i in range(1, 64):\n","            file_path = os.path.join(self.data_dir, f\"ProcessVar{i}.csv\")\n","            if not os.path.exists(file_path):\n","                raise FileNotFoundError(f\"File not found: {file_path}\")\n","            sensor_data = pd.read_csv(file_path, header=None).values.T  # Transpose to 300x1200\n","            all_data.append(sensor_data)\n","        return np.hstack(all_data)  # Combine all sensors column-wise\n","\n","    def __len__(self):\n","        return self.data.shape[0]\n","\n","    def __getitem__(self, idx):\n","        sample = self.data[idx, :]\n","        if self.transform:\n","            sample = self.transform(sample)\n","        return sample"],"metadata":{"id":"Yb884zqssdma"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Diffusion Model Setup\n","class DiffusionModel(torch.nn.Module):\n","    def __init__(self, input_dim, embed_dim=128):\n","        super(DiffusionModel, self).__init__()\n","        # Define a simple MLP for the diffusion model\n","        self.fc1 = torch.nn.Linear(input_dim, embed_dim)\n","        self.dropout = torch.nn.Dropout(0.2)\n","        self.fc2 = torch.nn.Linear(embed_dim, embed_dim)\n","        self.fc3 = torch.nn.Linear(embed_dim, input_dim)\n","\n","    def forward(self, x):\n","        x = torch.relu(self.fc1(x))\n","        x = self.dropout(x)\n","        x = torch.relu(self.fc2(x))\n","        return self.fc3(x)\n"],"metadata":{"id":"aadjX-VLsn_F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training and Evaluation Functions\n","def train_model(model, dataloader, optimizer, criterion, epochs, scheduler=None):\n","    model.train()\n","    for epoch in range(epochs):\n","        total_loss = 0\n","        for batch in dataloader:\n","            optimizer.zero_grad()\n","            outputs = model(batch.float())\n","            loss = criterion(outputs, batch.float())\n","            loss.backward()\n","            optimizer.step()\n","            total_loss += loss.item()\n","        if scheduler:\n","            scheduler.step()\n","        print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(dataloader):.4f}\")\n","\n","def evaluate_model(model, dataloader, criterion):\n","    model.eval()\n","    total_loss = 0\n","    with torch.no_grad():\n","        for batch in dataloader:\n","            outputs = model(batch.float())\n","            loss = criterion(outputs, batch.float())\n","            total_loss += loss.item()\n","    print(f\"Evaluation Loss: {total_loss / len(dataloader):.4f}\")\n"],"metadata":{"id":"NhESrNFxsrad"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Experiment: Missing Data Imputation\n","def run_experiment(model, dataset, missing_ratio=0.1):\n","    # Create a copy of the data\n","    data = dataset.data.copy()\n","\n","    # Mask 10% of the data arbitrarily\n","    num_samples, num_features = data.shape\n","    num_missing = int(missing_ratio * num_samples * num_features)\n","    missing_indices = np.random.choice(num_samples * num_features, num_missing, replace=False)\n","\n","    # Create a mask and apply it to the data\n","    mask = np.ones_like(data, dtype=bool)\n","    mask[np.unravel_index(missing_indices, data.shape)] = False\n","    corrupted_data = data * mask\n","\n","    # Use the trained model to generate missing parts\n","    model.eval()\n","    with torch.no_grad():\n","        generated_data = model(torch.tensor(corrupted_data).float()).numpy()\n","\n","    # Compute the discrepancy (e.g., RMSE) between real and generated parts\n","    real_missing_values = data[~mask]\n","    generated_missing_values = generated_data[~mask]\n","    rmse = np.sqrt(np.mean((real_missing_values - generated_missing_values) ** 2))\n","\n","    print(f\"RMSE for missing data imputation: {rmse:.4f}\")\n","\n","    # Enhanced visualization: plot real, masked, and generated data per sensor\n","    for sensor_idx in range(5):  # Display for the first 5 sensors\n","        plt.figure(figsize=(12, 4))\n","        plt.plot(data[sensor_idx], label=\"Real Data\", alpha=0.7, color=\"blue\")\n","        plt.plot(generated_data[sensor_idx], label=\"Generated Data\", linestyle=\"--\", alpha=0.7, color=\"green\")\n","        masked_points = np.where(~mask[sensor_idx])[0]\n","        plt.scatter(masked_points, data[sensor_idx][masked_points], color=\"red\", label=\"Masked Points\", alpha=0.8)\n","        plt.title(f\"Sensor {sensor_idx + 1}: Real vs Generated\")\n","        plt.xlabel(\"Time Steps\")\n","        plt.ylabel(\"Sensor Values\")\n","        plt.legend()\n","        plt.show()\n","\n","    return real_missing_values, generated_missing_values, rmse"],"metadata":{"id":"ysGo9ioqstmb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Additional Experiment Settings and Techniques\n","def setup_advanced_experiments(dataset, model, train_loader, test_loader, criterion):\n","    # Normalize Data\n","    scaler = StandardScaler()\n","    dataset.data = scaler.fit_transform(dataset.data)\n","    print(\"Data normalized.\")\n","\n","    # Experiment with Different Learning Rates\n","    print(\"Experiment: Reducing Learning Rate\")\n","    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n","    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n","    train_model(model, train_loader, optimizer, criterion, epochs=20, scheduler=scheduler)\n","    evaluate_model(model, test_loader, criterion)\n","\n","    # Experiment with Larger Batch Size\n","    print(\"Experiment: Increasing Batch Size\")\n","    train_loader = DataLoader(dataset, batch_size=64, shuffle=True)\n","    test_loader = DataLoader(dataset, batch_size=64, shuffle=False)\n","    train_model(model, train_loader, optimizer, criterion, epochs=20)\n","    evaluate_model(model, test_loader, criterion)"],"metadata":{"id":"A5W5XnW0xFnA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Compare Models\n","def compare_models(dataset, baseline_model, advanced_model):\n","    print(\"Comparing Baseline and Advanced Models...\")\n","    baseline_real, baseline_generated, baseline_rmse = run_experiment(baseline_model, dataset)\n","    advanced_real, advanced_generated, advanced_rmse = run_experiment(advanced_model, dataset)\n","\n","    print(f\"Baseline Model RMSE: {baseline_rmse:.4f}\")\n","    print(f\"Advanced Model RMSE: {advanced_rmse:.4f}\")\n","\n","    # Enhanced comparison of real, masked, and generated data for both models\n","    for sensor_idx in range(5):  # Compare for the first 5 sensors\n","        plt.figure(figsize=(12, 4))\n","        plt.plot(dataset.data[sensor_idx], label=\"Real Data\", alpha=0.7, color=\"blue\")\n","        plt.plot(baseline_generated[sensor_idx], label=\"Baseline Generated\", linestyle=\"--\", alpha=0.7, color=\"orange\")\n","        plt.plot(advanced_generated[sensor_idx], label=\"Advanced Generated\", linestyle=\"-.\", alpha=0.7, color=\"green\")\n","        masked_points = np.where(~np.isfinite(dataset.data[sensor_idx]))[0]\n","        plt.scatter(masked_points, dataset.data[sensor_idx][masked_points], color=\"red\", label=\"Masked Points\", alpha=0.8)\n","        plt.title(f\"Sensor {sensor_idx + 1}: Real vs Baseline vs Advanced\")\n","        plt.xlabel(\"Time Steps\")\n","        plt.ylabel(\"Sensor Values\")\n","        plt.legend()\n","        plt.show()"],"metadata":{"id":"ODkze3sazlWc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Main Execution\n","if __name__ == \"__main__\":\n","    # Mount Google Drive and extract the zip file\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    zip_path = \"/content/drive/MyDrive/Colab Notebooks/Generative_Deep_Learning_2nd_Edition/Generative_Deep_Learning_2nd_Edition/notebooks/08_diffusion/01_ddm/Sensor_data.zip\"\n","    extract_path = \"/content/Sensor_data\"\n","\n","    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","        zip_ref.extractall(extract_path)\n","\n","    # Load Dataset\n","    data_dir = extract_path\n","    dataset = SensorDataset(data_dir, transform=torch.tensor)\n","\n","    # Split into train and test sets\n","    train_size = int(0.8 * len(dataset))\n","    test_size = len(dataset) - train_size\n","    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n","\n","    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n","\n","    # Baseline Model\n","    input_dim = dataset[0].shape[0]\n","    baseline_model = DiffusionModel(input_dim)\n","    criterion = torch.nn.MSELoss()\n","    baseline_optimizer = torch.optim.Adam(baseline_model.parameters(), lr=1e-3)\n","\n","    train_model(baseline_model, train_loader, baseline_optimizer, criterion, epochs=20)\n","\n","    # Advanced Model with Scheduler\n","    advanced_model = DiffusionModel(input_dim)\n","    advanced_optimizer = torch.optim.Adam(advanced_model.parameters(), lr=1e-4, weight_decay=1e-5)\n","    scheduler = torch.optim.lr_scheduler.StepLR(advanced_optimizer, step_size=10, gamma=0.5)\n","\n","    train_model(advanced_model, train_loader, advanced_optimizer, criterion, epochs=20, scheduler=scheduler)\n","\n","    # Compare Models\n","    compare_models(dataset, baseline_model, advanced_model)\n"],"metadata":{"id":"yWCKR74Z76DB"},"execution_count":null,"outputs":[]}]}